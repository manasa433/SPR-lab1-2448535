{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-74seGUq09",
        "outputId": "dbaa60c8-0b7b-41d5-9b86-0e43c34df3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install libraries for both offline (Whisper) and online (Google) recognition\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import warnings\n",
        "\n",
        "# Suppress noisy warnings from the whisper library\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "audio_file_path = \"/content/lab3sample.wav\"\n",
        "\n",
        "#Offline Recognition with OpenAI's Whisper\n",
        "print(\"Starting Offline Recognition with Whisper\")\n",
        "try:\n",
        "    # Feedback: Let the user know recognition is in progress\n",
        "    print(\"Recognizing with Whisper...\")\n",
        "\n",
        "    # Load the base model (good balance of speed and accuracy)\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file_path, fp16=False)\n",
        "\n",
        "    # Display the recognized text\n",
        "    recognized_text = result[\"text\"].strip()\n",
        "    print(f\"Speech recognized: '{recognized_text}'\")\n",
        "\n",
        "    # Display success message\n",
        "    print(\"Speech successfully converted to text!\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Handle any other exceptions during Whisper processing\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator for clarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_B-o4P8Vxoz",
        "outputId": "b8c38182-5353-4466-d757-f6209d84d004"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Offline Recognition with Whisper\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech recognized: 'I believe you're just talking nonsense.'\n",
            "Speech successfully converted to text!\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Online Recognition with Google Speech API\n",
        "print(\"--- Starting Online Recognition with Google Speech API ---\")\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Use the audio file as the source\n",
        "with sr.AudioFile(audio_file_path) as source:\n",
        "    # Feedback: Let the user know what to do (though we are using a file)\n",
        "    print(\"Speak something... (using audio file as source)\")\n",
        "\n",
        "    # Read the audio data from the file\n",
        "    audio_data = r.record(source)\n",
        "\n",
        "    # Feedback: Let the user know recognition is in progress\n",
        "    print(\"Recognizing with Google API...\")\n",
        "\n",
        "    # Try to recognize the speech using Google's free Web Speech API\n",
        "    try:\n",
        "        # Convert speech to text\n",
        "        recognized_text = r.recognize_google(audio_data)\n",
        "\n",
        "        # Display the recognized text\n",
        "        print(f\"Speech recognized: '{recognized_text}'\")\n",
        "\n",
        "        # Display success message\n",
        "        print(\"Speech successfully converted to text!\")\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        # Handle unclear speech\n",
        "        print(\"Speech Recognition could not understand audio. Please try speaking more clearly.\")\n",
        "    except sr.RequestError as e:\n",
        "        # Handle service unavailability\n",
        "        print(f\"Could not request results from Google Speech Recognition service; {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB41YEIlWsEr",
        "outputId": "b854e126-a2cc-4cca-dc36-9eb337b8c837"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Online Recognition with Google Speech API ---\n",
            "Speak something... (using audio file as source)\n",
            "Recognizing with Google API...\n",
            "Speech recognized: 'I believe you are just talking nonsense'\n",
            "Speech successfully converted to text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMTCPGxJi8vE",
        "outputId": "bd47858e-8e28-490e-fa85-eaba04538851"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Female Voice"
      ],
      "metadata": {
        "id": "DAOGf8Vx-uQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import warnings\n",
        "import os\n",
        "import vosk\n",
        "import wave\n",
        "import json\n",
        "\n",
        "audio_file_path = \"/content/voice.wav\"\n",
        "\n",
        "# OFFLINE WITH WHISPER\n",
        "print(\"Transcribing with Whisper (Offline Model)\")\n",
        "try:\n",
        "    # Load the 'base' Whisper model\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file_path, fp16=False)\n",
        "\n",
        "    # Print the recognized text\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Visual separator\n",
        "\n",
        "# ONLINE WITH GOOGLE SPEECH API\n",
        "print(\" Transcribing with Google Speech API (Online Model) \")\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Process the audio file\n",
        "with sr.AudioFile(audio_file_path) as source:\n",
        "    # Read the audio data from the file\n",
        "    audio_data = r.record(source)\n",
        "\n",
        "    # Try to recognize the speech using Google's API\n",
        "    try:\n",
        "        # Send audio to Google for transcription\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "\n",
        "        # Print the recognized text\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        # This error happens if the API can't understand the audio\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        # This error happens if there's a problem with the network or the API service\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "# OFFLINE WITH VOSK\n",
        "print(\" Transcribing with Vosk (Offline Model) \")\n",
        "try:\n",
        "    # Check for and download the Vosk model\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        # Using shell commands which are simple and effective in Colab\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    # Load the Vosk model\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "\n",
        "    # Open the audio file in the format Vosk requires\n",
        "    wf = wave.open(audio_file_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    # Process the audio data in chunks\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    # Get the final result and parse the JSON output\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvzUYePIgNFB",
        "outputId": "9f02157d-c44b-4dcd-8cca-e849d3a00d6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing with Whisper (Offline Model)\n",
            "Whisper Output: 'Hi there. How are you doing? Take care. Be happy. Bye.'\n",
            "\n",
            "==================================================\n",
            "\n",
            " Transcribing with Google Speech API (Online Model) \n",
            "Google API Output: 'hi there how are you doing take care be happy bye'\n",
            " Transcribing with Vosk (Offline Model) \n",
            "Vosk model not found. Downloading 'vosk-model-small-en-us-0.15'...\n",
            "Model downloaded and unzipped successfully.\n",
            "Vosk Output: 'on him home one to to be oh boy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Male voice"
      ],
      "metadata": {
        "id": "ytEXU0hQ_CsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5B1AE08_EoB",
        "outputId": "8a0e7ccf-74e1-45b9-ac27-d955f67111f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/male Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "#AUDIO CONVERSION STEP (IMPROVED)\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# - OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "    # Use the new, standardized audio file\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nh1jrxVAGZ3",
        "outputId": "8030320f-30a1-4fb1-ba81-f3d21135032f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hi, hello, can I talk with Manasa? Is everything okay? Have a nice day, take care.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'hi hello can I talk with Mansa is everything okay have a nice day take care'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'hi hello can it don't commit an answer is everything okay i'm in a day daycare'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast speech"
      ],
      "metadata": {
        "id": "T5nNO4baAvRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "151T8nh2Ay1P",
        "outputId": "4575fcbb-6993-4244-c6d8-8c4e1b34f284"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/fast Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "# --- AUDIO CONVERSION STEP\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKRUXb0VD6Xz",
        "outputId": "755f11de-64c0-4045-b2b9-2e64987808d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hi everyone how are you doing? Take care. Bye.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'hi everyone how are you how are you doing take care bye'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'tyree when how are you are you doing they can buy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise Background"
      ],
      "metadata": {
        "id": "7UDPYLp_E0iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnBwClMJE3Mk",
        "outputId": "dfab54d0-b91b-4b54-c677-6064e52875d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/Noisy Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "#  AUDIO CONVERSION STEP (IMPROVED)\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "    # Use the new, standardized audio file\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KDzVCNoE9PY",
        "outputId": "7cd7f818-e2b3-4915-dc90-f1d086ddcfb3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hello everyone, good morning, welcome to my YouTube channel. Now I am doing the lab 3x series of SPR that speaks to text application. Thank you.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'tell everyone good morning welcome to my YouTube channel now I'm doing the lab 3x images of SBR that speech to text application thank you'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'hello everyone that money will come to my you tube channel know i'm doing the lab three acres of and be that it's been to fix to application thank you'\n"
          ]
        }
      ]
    }
  ]
}